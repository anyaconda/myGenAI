{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e833347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta 5/7/2024 Try Gemma \n",
    "#Goal: Code snippets w/ Gemma foundation model\n",
    "# started from https://huggingface.co/google/gemma-2b (weird), switched to https://huggingface.co/google/gemma-2b-it\n",
    "\n",
    "\n",
    "#infra: Trainbox + VSCode \n",
    "#      env: tc2-venv created by Blake\n",
    "#      confirmed Python 3.10.4\n",
    "#      numpy 1.24.2, pandas 2.0.0, added scikit-learn 1.2.2, req scipy\n",
    "#      [not any more pip 22.0.4], ipykernel 6.22.0, ipython 8.12.0\n",
    "#\n",
    "#for HuggingFace added\n",
    "#      tqdm-4.66.2 huggingface-hub-0.21.1 [safetensors-0.4.2 tokenizers-0.15.2] transformers-4.38.1\n",
    "#need a lower version `ulrlib3` -> downgrade it with $ pip install requests==2.27.1 -> urllib3 1.26.18\n",
    "#needs pip 24.0\n",
    "#for HuggingFace Sentence Embeddings added\n",
    "#      sentence-transformers-2.5.1\n",
    "\n",
    "\n",
    "#input: n/a \n",
    "#output: n/a\n",
    "\n",
    "\n",
    "#history\n",
    "#5/7/2024 TRY HF GEMMA \n",
    "#      2B instruct version of the Gemma model \"google/gemma-2b-it\"\n",
    "\n",
    "#$config $private\n",
    "\n",
    "#References\n",
    "#HF Gemma\n",
    "# https://huggingface.co/google/gemma-2b-it\n",
    "\n",
    "#Getting Started with Google’s Gemma LLM using HuggingFace Libraries  \n",
    "# https://medium.com/@coldstart_coder/getting-started-with-googles-gemma-llm-using-huggingface-libraries-a0d826c552ae#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d20cb5",
   "metadata": {},
   "source": [
    "import os \n",
    "import sys\n",
    "import time as time\n",
    "import numpy as np\n",
    "\n",
    "#ML\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628fc7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]\n",
      "1.24.2\n"
     ]
    }
   ],
   "source": [
    "#python version $my\n",
    "print(sys.version)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f72a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----- GlOBAL VARS -----\n",
    "###hf_token = #\"get your token in http://hf.co/settings/tokens\" $private\n",
    "\n",
    "# identify which checkpoint we want, this is the repository on huggingface that we'll pull the model from\n",
    "MODEL_CHECKPOINT = \"google/gemma-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c1bfcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$config network settings\n",
    "\n",
    "#set proxy needed to fix SSL error\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '../../myCreds/all_ca_certs.crt'\n",
    "# #os.environ['CURL_CA_BUNDLE'] = '../../myCreds/all_ca_certs.pem'\n",
    "\n",
    "proxy = 'http://url:8080' #$config\n",
    "os.environ['http_proxy'] = proxy \n",
    "os.environ['HTTP_PROXY'] = proxy\n",
    "os.environ['https_proxy'] = proxy\n",
    "os.environ['HTTPS_PROXY'] = proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c2103",
   "metadata": {},
   "source": [
    "# Try Gemma\n",
    "Using HF model https://huggingface.co/google/gemma-2b-it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f981824",
   "metadata": {},
   "source": [
    "## Step 2. Model   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ead18b2",
   "metadata": {},
   "source": [
    "### Load a pre-trained LM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76513c",
   "metadata": {},
   "source": [
    "How to: Access gated repo issue!  \n",
    "Refer to https://huggingface.co/google/gemma-2b/discussions/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "124e3c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, they weave and they learn,\n",
      "From the data, they discern.\n",
      "Algorithms, a symphony,\n",
      "Unleash the power of the machine.\n",
      "\n",
      "With each iteration, they grow,\n",
      "A tapestry of insights they sow.\n",
      "From the past, they learn and they adapt,\n",
      "A never-ending, ever-fast.\n",
      "\n",
      "The future holds mysteries untold,\n",
      "Where machines will shape our world.\n",
      "They will heal, they\n"
     ]
    }
   ],
   "source": [
    "# load in the tokenizer,\n",
    "# just in case you aren't familiar with llms the tokenizer takes the raw text and converts it into a vector that can be processed by our model,\n",
    "# so basically it's a converter to convert english into numbers for our model to compute on\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT, token=hf_token)\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\") \n",
    "\n",
    "outputs = model.generate(**input_ids, max_length = 100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "124e3c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><pad><bos>I love sushi! But I'm not a seasoned sushi chef, and I'm not sure where to start.\n",
      "\n",
      "**What's the best way to learn to make sushi at home?**\n",
      "<bos>I've been waiting for this blog my whole life. It's finally here, and I'm so excited to read it.\n",
      "\n",
      "I've been searching for a blog that truly understands the struggles and joys of life, and I\n"
     ]
    }
   ],
   "source": [
    "#multi input\n",
    "input_text = [\n",
    "    \"I love sushi!\",\n",
    "    \"I've been waiting for this blog my whole life.\",\n",
    "]\n",
    "input_ids = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "\n",
    "outputs = model.generate(**input_ids, max_length = 50)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "print(tokenizer.decode(outputs[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a8b48",
   "metadata": {},
   "source": [
    "### Additional Gen Parameters\n",
    "Example: Getting Started with Google’s Gemma LLM using HuggingFace Libraries  \n",
    "https://medium.com/@coldstart_coder/getting-started-with-googles-gemma-llm-using-huggingface-libraries-a0d826c552ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88bba68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, with gears and wires,\n",
      "Unleash a power that's beyond compare.\n",
      "Algorithms dance, a symphony,\n",
      "Learning from data, setting a new free.\n",
      "\n",
      "From medical scans to social media's sway,\n",
      "They shape our world, come what may.\n",
      "With every query, a new insight unfolds,\n",
      "A world of possibilities, a story to be told.\n",
      "\n",
      "But be warned, the path is not without blight,\n",
      "Bias and discrimination, a dark night.\n",
      "Ethical concerns, a constant fight,\n",
      "To ensure fairness, a beacon of light.\n",
      "\n",
      "So let us embrace this wondrous age,\n",
      "Where machines and humans, hand in hand, engage.\n",
      "With every step, we learn and grow,\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\") \n",
    "\n",
    "token_outputs = model.generate(**input_ids, max_new_tokens=150, do_sample=True, temperature=0.5)\n",
    "# since this is exploratory we'll decode special tokens as well. for this prompt\n",
    "# it will be the beginning <bos> token and the ending <eos> tokens.\n",
    "\n",
    "print(tokenizer.decode(token_outputs[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64774430",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mystop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmystop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mystop' is not defined"
     ]
    }
   ],
   "source": [
    "mystop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0c0ab",
   "metadata": {},
   "source": [
    "# Xtra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84081d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name google/gemma-2b. Creating a new one with MEAN pooling.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\LM load time: in 0.26163914(min): \n"
     ]
    }
   ],
   "source": [
    "#$xtra would it work as a sentence transformer?\n",
    "# not Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#from sentence_transformers import util\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "hf_model = SentenceTransformer('google/gemma-2b') #google-bert/bert-base-uncased') #\"all-MiniLM-L12-v2\") #\"wvprevue/e5-mistral-7b-instruct\") #all-MiniLM-L6-v2\n",
    "\n",
    "print (\"\\LM load time: in {:.8f}(min): \".format( (time.time() - t0)/60 ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
